{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "split-aluminum",
      "metadata": {
        "id": "split-aluminum",
        "papermill": {
          "duration": 0.048394,
          "end_time": "2021-04-15T21:06:39.560571",
          "exception": false,
          "start_time": "2021-04-15T21:06:39.512177",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# LAB | Extractive Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prospective-turner",
      "metadata": {
        "id": "prospective-turner",
        "papermill": {
          "duration": 0.045573,
          "end_time": "2021-04-15T21:06:39.651272",
          "exception": false,
          "start_time": "2021-04-15T21:06:39.605699",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "This notebook demonstrates how Pinecone helps you build an extractive question-answering application. To build an extractive question-answering system, we need three main components:\n",
        "\n",
        "- A vector index to store and run semantic search\n",
        "- A retriever model for embedding context passages\n",
        "- A reader model to extract answers\n",
        "\n",
        "We will use the SQuAD dataset, which consists of **questions** and **context** paragraphs containing question **answers**. We generate embeddings for the context passages using the retriever, index them in the vector database, and query with semantic search to retrieve the top k most relevant contexts containing potential answers to our question. We then use the reader model to extract the answers from the returned contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oC3GG-dWkZJ6",
      "metadata": {
        "id": "oC3GG-dWkZJ6"
      },
      "source": [
        "Let's get started by installing the packages needed for notebook to run:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5924add8",
      "metadata": {},
      "source": [
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "XFkQCVF8uNL3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFkQCVF8uNL3",
        "outputId": "720f709b-d5ee-4e99-c391-32782d27335b"
      },
      "outputs": [],
      "source": [
        "#!pip install -U langchain langchain-core langchain-classic langchain-pinecone langchain-huggingface datasets pinecone-client sentence-transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ZheZeF_WsSge",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZheZeF_WsSge",
        "outputId": "d0321891-5596-496c-86c7-c8cd345eb738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone API Key (masked): pcsk...c6qE\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Clear PINECONE_API_KEY from environment to ensure re-prompting if needed\n",
        "if \"PINECONE_API_KEY\" in os.environ:\n",
        "    del os.environ[\"PINECONE_API_KEY\"]\n",
        "\n",
        "# Prompt for API key if not set\n",
        "if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "    api_key_input = getpass.getpass(\"Enter your Pinecone API key: \")\n",
        "    os.environ[\"PINECONE_API_KEY\"] = api_key_input\n",
        "\n",
        "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "# Verify the key is set (print masked version)\n",
        "if pinecone_api_key:\n",
        "    print(f\"Pinecone API Key (masked): {pinecone_api_key[:4]}...{pinecone_api_key[-4:]}\")\n",
        "else:\n",
        "    print(\"Pinecone API Key is not set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29ad3840",
      "metadata": {
        "id": "29ad3840"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hgIieQukgagu",
      "metadata": {
        "id": "hgIieQukgagu"
      },
      "source": [
        "Now let's load the SQUAD dataset from the HuggingFace Model Hub. We load the dataset into a pandas dataframe and filter the title, question, and context columns, and we drop any duplicate context passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "J250IJeh7NIb",
      "metadata": {
        "id": "J250IJeh7NIb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13f57dc0bc7640bfbdc5c27513fb53ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python\\Python314\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\g\\.cache\\huggingface\\hub\\datasets--squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f89f2977d00e4b34a358bb4fd308bc65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3810f22c1750413a91aed6a83fc6868c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "plain_text/validation-00000-of-00001.par(…):   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e89a0499dd3646a99849f1ba3cae39c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1b11d465eb841bba91a0be3de7c87a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# load the squad dataset into a pandas dataframe\n",
        "df = load_dataset(\"squad\", split=\"train\").to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "FcmeNO97dHDO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "FcmeNO97dHDO",
        "outputId": "c89ac308-270d-4d1c-ff81-881daa98f13f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>As at most other universities, Notre Dame's st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>The university is the major seat of the Congre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>The College of Engineering was established in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>University_of_Notre_Dame</td>\n",
              "      <td>All of Notre Dame's undergraduate students are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87574</th>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Institute of Medicine, the central college of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87579</th>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Football and Cricket are the most popular spor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87584</th>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The total length of roads in Nepal is recorded...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87589</th>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>The main international airport serving Kathman...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87594</th>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18891 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                          title  \\\n",
              "0      University_of_Notre_Dame   \n",
              "5      University_of_Notre_Dame   \n",
              "10     University_of_Notre_Dame   \n",
              "15     University_of_Notre_Dame   \n",
              "20     University_of_Notre_Dame   \n",
              "...                         ...   \n",
              "87574                 Kathmandu   \n",
              "87579                 Kathmandu   \n",
              "87584                 Kathmandu   \n",
              "87589                 Kathmandu   \n",
              "87594                 Kathmandu   \n",
              "\n",
              "                                                 context  \n",
              "0      Architecturally, the school has a Catholic cha...  \n",
              "5      As at most other universities, Notre Dame's st...  \n",
              "10     The university is the major seat of the Congre...  \n",
              "15     The College of Engineering was established in ...  \n",
              "20     All of Notre Dame's undergraduate students are...  \n",
              "...                                                  ...  \n",
              "87574  Institute of Medicine, the central college of ...  \n",
              "87579  Football and Cricket are the most popular spor...  \n",
              "87584  The total length of roads in Nepal is recorded...  \n",
              "87589  The main international airport serving Kathman...  \n",
              "87594  Kathmandu Metropolitan City (KMC), in order to...  \n",
              "\n",
              "[18891 rows x 2 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# select only title and context column\n",
        "df = df[['title', 'context']]\n",
        "# drop rows containing duplicate context passages\n",
        "df = df.drop_duplicates(subset='context')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57bbcb57",
      "metadata": {
        "id": "57bbcb57"
      },
      "source": [
        "# Initialize Pinecone Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e24d904c",
      "metadata": {
        "id": "e24d904c"
      },
      "source": [
        "The Pinecone index stores vector representations of our context passages which we can retrieve using another vector (query vector). We first need to initialize our connection to Pinecone to create our vector index. For this, we need a free [API key](\"https://app.pinecone.io/\"), and then we initialize the connection like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "092d1e71",
      "metadata": {
        "id": "092d1e71"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\", region=\"us-east-1\"\n",
        ")\n",
        "\n",
        "# connect to pinecone environment\n",
        "pc = Pinecone(api_key=pinecone_api_key, environment=spec.region)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58028e12",
      "metadata": {
        "id": "58028e12"
      },
      "source": [
        "Now we create a new index called \"question-answering\" — we can name the index anything we want. We specify the metric type as \"cosine\" and dimension as 384 because the retriever we use to generate context embeddings is optimized for cosine similarity and outputs 384-dimension vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b3206184",
      "metadata": {
        "id": "b3206184"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
              "                                    'content-length': '150',\n",
              "                                    'content-type': 'application/json',\n",
              "                                    'date': 'Fri, 27 Feb 2026 09:56:23 GMT',\n",
              "                                    'grpc-status': '0',\n",
              "                                    'server': 'envoy',\n",
              "                                    'x-envoy-upstream-service-time': '44',\n",
              "                                    'x-pinecone-request-latency-ms': '43',\n",
              "                                    'x-pinecone-response-duration-ms': '45'}},\n",
              " 'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'memoryFullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {},\n",
              " 'storageFullness': 0.0,\n",
              " 'total_vector_count': 0,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_name = \"extractive-question-answering\"\n",
        "\n",
        "# Check if index exists & create with ServerlessSpec\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,\n",
        "        metric=\"cosine\",\n",
        "        spec=spec\n",
        "    )\n",
        "\n",
        "# Connect to index\n",
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e84a3e5",
      "metadata": {
        "id": "6e84a3e5"
      },
      "source": [
        "# Initialize Retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oZzhGS1Lpj0g",
      "metadata": {
        "id": "oZzhGS1Lpj0g"
      },
      "source": [
        "Next, we need to initialize our retriever. The retriever will mainly do two things:\n",
        "\n",
        "- Generate embeddings for all context passages (context vectors/embeddings)\n",
        "- Generate embeddings for our questions (query vector/embedding)\n",
        "\n",
        "The retriever will generate embeddings in a way that the questions and context passages containing answers to our questions are nearby in the vector space. We can use cosine similarity to calculate the similarity between the query and context embeddings to find the context passages that contain potential answers to our question.\n",
        "\n",
        "We will use a SentenceTransformer model named ``multi-qa-MiniLM-L6-cos-v1`` designed for semantic search and trained on 215M (question, answer) pairs from diverse sources as our retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16af336e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16af336e",
        "outputId": "71cfe387-997d-4d7f-e244-c245e70144e9"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers torch langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1e491882",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "PyTorch GPU Check\n",
            "==================================================\n",
            "PyTorch version: 2.10.0+cu130\n",
            "CUDA available: True\n",
            "CUDA version: 13.0\n",
            "GPU count: 1\n",
            "GPU name: NVIDIA GeForce RTX 2080\n",
            "GPU memory: 8.6 GB\n",
            "\n",
            "Test tensor on GPU: cuda:0 ✓\n"
          ]
        }
      ],
      "source": [
        "# GPU Check for PyTorch (HuggingFace/BERT)\n",
        "import torch\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"PyTorch GPU Check\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. PyTorch version\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# 2. CUDA available?\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# 3. If CUDA available, show details\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    \n",
        "    # Quick test\n",
        "    x = torch.rand(3, 3).cuda()\n",
        "    print(f\"\\nTest tensor on GPU: {x.device} ✓\")\n",
        "else:\n",
        "    print(\"\\n⚠️ CUDA NOT available - will run on CPU (very slow for BERT)\")\n",
        "    print(\"You need to install PyTorch with CUDA support\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "94d167c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94d167c8",
        "outputId": "daa7fd2d-d62e-4952-8d53-097f1362b341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75040b9364784a358b30b4290b0dae26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python\\Python314\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\g\\.cache\\huggingface\\hub\\models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5babff9d5dde41a3b661e81e46a2df71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8163238eb6cc4c878354b351d6d6e271",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47fa2acb6f434d6780383796eae9b9d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb3e8d4e506d43bf81000e45330e06f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab1cc0601adb44a58f49a972f35987ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3092f30bcd694264bed50436f5221e63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b914bf09b29a4e27bf7f1e8ad12603e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25ec207adfde4004aeacb42eae6430c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13a85032cc5743ba80e92e60edaacc7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0cc8eef01744788bfad8981c8ca051d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retriever model loaded: SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load retriever model (multi-qa-MiniLM-L6-cos-v1 - 384 dimensions)\n",
        "retriever = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device=device) #use the 'multi-qa-MiniLM-L6-cos-v1' model from HuggingFace to build the retriever\n",
        "\n",
        "# ✅ Now retriever is loaded and ready\n",
        "print(\"Retriever model loaded:\", retriever)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aaad0a2",
      "metadata": {
        "id": "8aaad0a2"
      },
      "source": [
        "# Generate Embeddings and Upsert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hgy7AagJtO_p",
      "metadata": {
        "id": "Hgy7AagJtO_p"
      },
      "source": [
        "Next, we need to generate embeddings for the context passages. We will do this in batches to help us more quickly generate embeddings and upload them to the Pinecone index. When passing the documents to Pinecone, we need an id (a unique value), context embedding, and metadata for each document representing context passages in the dataset. The metadata is a dictionary containing data relevant to our embeddings, such as the article title, context passage, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a17824ef",
      "metadata": {
        "id": "a17824ef",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "791ced33262d4701918f15a55a35d76c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/296 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
              "                                    'content-length': '189',\n",
              "                                    'content-type': 'application/json',\n",
              "                                    'date': 'Fri, 27 Feb 2026 10:07:30 GMT',\n",
              "                                    'grpc-status': '0',\n",
              "                                    'server': 'envoy',\n",
              "                                    'x-envoy-upstream-service-time': '5',\n",
              "                                    'x-pinecone-request-latency-ms': '4',\n",
              "                                    'x-pinecone-response-duration-ms': '7'}},\n",
              " 'dimension': 384,\n",
              " 'index_fullness': 0.0,\n",
              " 'memoryFullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {'__default__': {'vector_count': 18891}},\n",
              " 'storageFullness': 0.0,\n",
              " 'total_vector_count': 18891,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "for i in tqdm(range(0, len(df), batch_size)):\n",
        "    # find end of batch\n",
        "    i_end = min(i + batch_size, len(df))\n",
        "    # extract batch\n",
        "    batch = df.iloc[i:i_end]\n",
        "    # generate embeddings for batch\n",
        "    emb = retriever.encode(batch['context'].tolist()).tolist()\n",
        "    # get metadata\n",
        "    meta = batch.to_dict(orient='records')\n",
        "    # create unique IDs\n",
        "    ids = [str(x) for x in range(i, i_end)]\n",
        "    # add all to upsert list\n",
        "    to_upsert = list(zip(ids, emb, meta))\n",
        "    # upsert/insert these records to pinecone\n",
        "    index.upsert(vectors=to_upsert)\n",
        "\n",
        "# check that we have all vectors in index\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YFyBYafuJ0y0",
      "metadata": {
        "id": "YFyBYafuJ0y0"
      },
      "source": [
        "# Initialize Reader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HgdiLCz5ynOk",
      "metadata": {
        "id": "HgdiLCz5ynOk"
      },
      "source": [
        "We use the `deepset/electra-base-squad2` model from the HuggingFace model hub as our reader model. We load this model into a \"question-answering\" pipeline from HuggingFace transformers and feed it our questions and context passages individually. The model gives a prediction for each context we pass through the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "hg9XTDkIJzH_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hg9XTDkIJzH_",
        "outputId": "dd081211-037d-4363-f835-8abd563e01fc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "680d24a7966148f18d06a1f7f0a4da6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/635 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python\\Python314\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\g\\.cache\\huggingface\\hub\\models--deepset--electra-base-squad2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a46d35ed91fe4b71922cee653c85b44e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "421883c2ff164334a89d869f7769e890",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afaecdf1a0274abc82af2175595cf3fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df559faf2d63465faf4e9c4652808164",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x242bf271fd0>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_name = 'deepset/electra-base-squad2'\n",
        "# load the reader model into a question-answering pipeline\n",
        "reader = pipeline(tokenizer=model_name, model=model_name, task='question-answering', device=device)\n",
        "reader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14d89d6",
      "metadata": {
        "id": "e14d89d6"
      },
      "source": [
        "Now all the components we need are ready. Let's write some helper functions to execute our queries. The `get_context` function retrieves the context embeddings containing answers to our question from the Pinecone index, and the `extract_answer` function extracts the answers from these context passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c190cd87",
      "metadata": {},
      "outputs": [],
      "source": [
        "# gets context passages from the pinecone index\n",
        "def get_context(question, top_k):\n",
        "    # generate embeddings for the question\n",
        "    xq = retriever.encode(question).tolist()\n",
        "    # search pinecone index for context passage with the answer\n",
        "    xc = index.query(vector=xq, top_k=top_k, include_metadata=True)\n",
        "    # extract the context passage from pinecone search result\n",
        "    c = [m['metadata']['context'] for m in xc['matches']]\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9b7780b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# extracts answer from the context passage\n",
        "def extract_answer(question, context):\n",
        "    results = []\n",
        "    for c in context:\n",
        "        # feed the reader the question and contexts to extract answers\n",
        "        answer = reader(question=question, context=c)\n",
        "        # add the context to answer dict for printing both together\n",
        "        answer[\"context\"] = c\n",
        "        results.append(answer)\n",
        "    # sort the result based on the score from reader model\n",
        "    sorted_result = pprint(sorted(results, key=lambda x: x['score'], reverse=True))\n",
        "    return sorted_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5E3a3dkJ5ZQD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E3a3dkJ5ZQD",
        "outputId": "82000ee3-f103-49cf-f32b-c0abad3b63f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of natural gas (in 2013), which makes Egypt as the largest oil producer not member of the Organization of the Petroleum Exporting Countries (OPEC) and the second-largest dry natural gas producer in Africa. In 2013, Egypt was the largest consumer of oil and natural gas in Africa, as more than 20% of total oil consumption and more than 40% of total dry natural gas consumption in Africa. Also, Egypt possesses the largest oil refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is currently planning to build its first nuclear power plant in El Dabaa city, northern Egypt.',\n",
              " 'Egypt has a developed energy market based on coal, oil, natural gas, and hydro power. Substantial coal deposits in the northeast Sinai are mined at the rate of about 600,000 tonnes (590,000 long tons; 660,000 short tons) per year. Oil and gas are produced in the western desert regions, the Gulf of Suez, and the Nile Delta. Egypt has huge reserves of gas, estimated at 2,180 cubic kilometres (520 cu mi), and LNG up to 2012 exported to many countries. In 2013, the Egyptian General Petroleum Co (EGPC) said the country will cut exports of natural gas and tell major industries to slow output this summer to avoid an energy crisis and stave off political unrest, Reuters has reported. Egypt is counting on top liquid natural gas (LNG) exporter Qatar to obtain additional gas volumes in summer, while encouraging factories to plan their annual maintenance for those months of peak demand, said EGPC chairman, Tarek El Barkatawy. Egypt produces its own energy, but has been a net oil importer since 2008 and is rapidly becoming a net importer of natural gas.',\n",
              " \"Iran has the second largest proved gas reserves in the world after Russia, with 33.6 trillion cubic metres, and third largest natural gas production in the world after Indonesia, and Russia. It also ranks fourth in oil reserves with an estimated 153,600,000,000 barrels. It is OPEC's 2nd largest oil exporter and is an energy superpower. In 2005, Iran spent US$4 billion on fuel imports, because of contraband and inefficient domestic use. Oil industry output averaged 4 million barrels per day (640,000 m3/d) in 2005, compared with the peak of six million barrels per day reached in 1974. In the early years of the 2000s (decade), industry infrastructure was increasingly inefficient because of technological lags. Few exploratory wells were drilled in 2005.\"]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"How much oil is Egypt producing in a day?\"\n",
        "context = get_context(question, top_k = 3)\n",
        "context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "heKNVbWQ_LtC",
      "metadata": {
        "id": "heKNVbWQ_LtC"
      },
      "source": [
        "As we can see, the retiever is working fine and gets us the context passage that contains the answer to our question. Now let's use the reader to extract the exact answer from the context passage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "DQ4GWdbMSjPl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ4GWdbMSjPl",
        "outputId": "5f0d30d9-5803-4bc7-e98f-671e0a0bb603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'answer': '691,000 bbl/d',\n",
            "  'context': 'Egypt was producing 691,000 bbl/d of oil and 2,141.05 Tcf of '\n",
            "             'natural gas (in 2013), which makes Egypt as the largest oil '\n",
            "             'producer not member of the Organization of the Petroleum '\n",
            "             'Exporting Countries (OPEC) and the second-largest dry natural '\n",
            "             'gas producer in Africa. In 2013, Egypt was the largest consumer '\n",
            "             'of oil and natural gas in Africa, as more than 20% of total oil '\n",
            "             'consumption and more than 40% of total dry natural gas '\n",
            "             'consumption in Africa. Also, Egypt possesses the largest oil '\n",
            "             'refinery capacity in Africa 726,000 bbl/d (in 2012). Egypt is '\n",
            "             'currently planning to build its first nuclear power plant in El '\n",
            "             'Dabaa city, northern Egypt.',\n",
            "  'end': 33,\n",
            "  'score': 0.9999855201975834,\n",
            "  'start': 20},\n",
            " {'answer': '600,000 tonnes',\n",
            "  'context': 'Egypt has a developed energy market based on coal, oil, natural '\n",
            "             'gas, and hydro power. Substantial coal deposits in the northeast '\n",
            "             'Sinai are mined at the rate of about 600,000 tonnes (590,000 '\n",
            "             'long tons; 660,000 short tons) per year. Oil and gas are '\n",
            "             'produced in the western desert regions, the Gulf of Suez, and '\n",
            "             'the Nile Delta. Egypt has huge reserves of gas, estimated at '\n",
            "             '2,180 cubic kilometres (520 cu mi), and LNG up to 2012 exported '\n",
            "             'to many countries. In 2013, the Egyptian General Petroleum Co '\n",
            "             '(EGPC) said the country will cut exports of natural gas and tell '\n",
            "             'major industries to slow output this summer to avoid an energy '\n",
            "             'crisis and stave off political unrest, Reuters has reported. '\n",
            "             'Egypt is counting on top liquid natural gas (LNG) exporter Qatar '\n",
            "             'to obtain additional gas volumes in summer, while encouraging '\n",
            "             'factories to plan their annual maintenance for those months of '\n",
            "             'peak demand, said EGPC chairman, Tarek El Barkatawy. Egypt '\n",
            "             'produces its own energy, but has been a net oil importer since '\n",
            "             '2008 and is rapidly becoming a net importer of natural gas.',\n",
            "  'end': 180,\n",
            "  'score': 4.702655972965708e-11,\n",
            "  'start': 166},\n",
            " {'answer': 'six million barrels per day',\n",
            "  'context': 'Iran has the second largest proved gas reserves in the world '\n",
            "             'after Russia, with 33.6 trillion cubic metres, and third largest '\n",
            "             'natural gas production in the world after Indonesia, and Russia. '\n",
            "             'It also ranks fourth in oil reserves with an estimated '\n",
            "             \"153,600,000,000 barrels. It is OPEC's 2nd largest oil exporter \"\n",
            "             'and is an energy superpower. In 2005, Iran spent US$4 billion on '\n",
            "             'fuel imports, because of contraband and inefficient domestic '\n",
            "             'use. Oil industry output averaged 4 million barrels per day '\n",
            "             '(640,000 m3/d) in 2005, compared with the peak of six million '\n",
            "             'barrels per day reached in 1974. In the early years of the 2000s '\n",
            "             '(decade), industry infrastructure was increasingly inefficient '\n",
            "             'because of technological lags. Few exploratory wells were '\n",
            "             'drilled in 2005.',\n",
            "  'end': 572,\n",
            "  'score': 8.683906892181792e-12,\n",
            "  'start': 545}]\n"
          ]
        }
      ],
      "source": [
        "extract_answer(question, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fMD_ABuDAyhN",
      "metadata": {
        "id": "fMD_ABuDAyhN"
      },
      "source": [
        "The reader model predicted with 99% accuracy the correct answer *691,000 bbl/d* as seen from the context passage. Let's run few more queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "_4NRgV4mGWoj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4NRgV4mGWoj",
        "outputId": "d27c41a5-8b71-4abc-97d5-dee6e49a4126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'answer': 'Hurley and Chen',\n",
            "  'context': 'According to a story that has often been repeated in the media, '\n",
            "             'Hurley and Chen developed the idea for YouTube during the early '\n",
            "             'months of 2005, after they had experienced difficulty sharing '\n",
            "             \"videos that had been shot at a dinner party at Chen's apartment \"\n",
            "             'in San Francisco. Karim did not attend the party and denied that '\n",
            "             'it had occurred, but Chen commented that the idea that YouTube '\n",
            "             'was founded after a dinner party \"was probably very strengthened '\n",
            "             'by marketing ideas around creating a story that was very '\n",
            "             'digestible\".',\n",
            "  'end': 79,\n",
            "  'score': 0.9999276399612427,\n",
            "  'start': 64}]\n"
          ]
        }
      ],
      "source": [
        "question = \"What are the first names of the men that invented youtube?\"\n",
        "context = get_context(question, top_k=1)\n",
        "extract_answer(question, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "juXlctWgJgMF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juXlctWgJgMF",
        "outputId": "046ed04d-47db-41e8-fc2b-2580a018540f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'answer': 'his theories of special relativity and general relativity',\n",
            "  'context': 'Albert Einstein is known for his theories of special relativity '\n",
            "             'and general relativity. He also made important contributions to '\n",
            "             'statistical mechanics, especially his mathematical treatment of '\n",
            "             'Brownian motion, his resolution of the paradox of specific '\n",
            "             'heats, and his connection of fluctuations and dissipation. '\n",
            "             'Despite his reservations about its interpretation, Einstein also '\n",
            "             'made contributions to quantum mechanics and, indirectly, quantum '\n",
            "             'field theory, primarily through his theoretical studies of the '\n",
            "             'photon.',\n",
            "  'end': 86,\n",
            "  'score': 0.9500377774238586,\n",
            "  'start': 29}]\n"
          ]
        }
      ],
      "source": [
        "question = \"What is Albert Eistein famous for?\"\n",
        "context = get_context(question, top_k=1)\n",
        "extract_answer(question, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OhCgeny_BVno",
      "metadata": {
        "id": "OhCgeny_BVno"
      },
      "source": [
        "Let's run another question. This time for top 3 context passages from the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "iXACn71xmett",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXACn71xmett",
        "outputId": "b0fcb675-0617-4faf-e184-c0dd4e2df70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'answer': 'Armstrong',\n",
            "  'context': 'The trip to the Moon took just over three days. After achieving '\n",
            "             'orbit, Armstrong and Aldrin transferred into the Lunar Module, '\n",
            "             'named Eagle, and after a landing gear inspection by Collins '\n",
            "             'remaining in the Command/Service Module Columbia, began their '\n",
            "             'descent. After overcoming several computer overload alarms '\n",
            "             'caused by an antenna switch left in the wrong position, and a '\n",
            "             'slight downrange error, Armstrong took over manual flight '\n",
            "             'control at about 180 meters (590 ft), and guided the Lunar '\n",
            "             'Module to a safe landing spot at 20:18:04 UTC, July 20, 1969 '\n",
            "             '(3:17:04 pm CDT). The first humans on the Moon would wait '\n",
            "             'another six hours before they ventured out of their craft. At '\n",
            "             '02:56 UTC, July 21 (9:56 pm CDT July 20), Armstrong became the '\n",
            "             'first human to set foot on the Moon.',\n",
            "  'end': 80,\n",
            "  'score': 0.9998037815093994,\n",
            "  'start': 71},\n",
            " {'answer': 'Aldrin',\n",
            "  'context': 'The first step was witnessed by at least one-fifth of the '\n",
            "             'population of Earth, or about 723 million people. His first '\n",
            "             \"words when he stepped off the LM's landing footpad were, \"\n",
            "             '\"That\\'s one small step for [a] man, one giant leap for '\n",
            "             'mankind.\" Aldrin joined him on the surface almost 20 minutes '\n",
            "             'later. Altogether, they spent just under two and one-quarter '\n",
            "             'hours outside their craft. The next day, they performed the '\n",
            "             'first launch from another celestial body, and rendezvoused back '\n",
            "             'with Columbia.',\n",
            "  'end': 246,\n",
            "  'score': 0.7011039354838431,\n",
            "  'start': 240},\n",
            " {'answer': 'Frank Borman',\n",
            "  'context': 'On December 21, 1968, Frank Borman, James Lovell, and William '\n",
            "             'Anders became the first humans to ride the Saturn V rocket into '\n",
            "             'space on Apollo 8. They also became the first to leave low-Earth '\n",
            "             'orbit and go to another celestial body, and entered lunar orbit '\n",
            "             'on December 24. They made ten orbits in twenty hours, and '\n",
            "             'transmitted one of the most watched TV broadcasts in history, '\n",
            "             'with their Christmas Eve program from lunar orbit, that '\n",
            "             'concluded with a reading from the biblical Book of Genesis. Two '\n",
            "             'and a half hours after the broadcast, they fired their engine to '\n",
            "             'perform the first trans-Earth injection to leave lunar orbit and '\n",
            "             'return to the Earth. Apollo 8 safely landed in the Pacific ocean '\n",
            "             \"on December 27, in NASA's first dawn splashdown and recovery.\",\n",
            "  'end': 34,\n",
            "  'score': 0.5020950753241777,\n",
            "  'start': 22}]\n"
          ]
        }
      ],
      "source": [
        "question = \"Who was the first person to step foot on the moon?\"\n",
        "context = get_context(question, top_k=3)\n",
        "extract_answer(question, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9oCWHy0tPpV",
      "metadata": {
        "id": "c9oCWHy0tPpV"
      },
      "source": [
        "The result looks pretty good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d83f9f55-b099-4280-a12e-1d0192f4f5aa",
      "metadata": {
        "id": "d83f9f55-b099-4280-a12e-1d0192f4f5aa"
      },
      "outputs": [],
      "source": [
        "pc.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27fecadc-6976-43b2-90e1-788a8633ecc7",
      "metadata": {
        "id": "27fecadc-6976-43b2-90e1-788a8633ecc7"
      },
      "source": [
        "### Add a few more questions. What did you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5eeed68",
      "metadata": {},
      "source": [
        "### Observations:\n",
        "### 1. High confidence (99%+) when answer exists clearly in context\n",
        "### 2. Extractive QA can only return text present word-for-word in the dataset\n",
        "### 3. With top_k=3, reader may extract different answers from different passages - sorting by score helps identify the best one\n",
        "### 4. Model extracts what's in context, not what's asked - e.g. surnames instead of first names\n",
        "### 5. Fast and transparent - you can always trace where the answer came from"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "environment": {
      "name": "tf2-gpu.2-3.m65",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 333.240754,
      "end_time": "2021-04-15T21:12:11.363566",
      "environment_variables": {},
      "exception": null,
      "input_path": "/notebooks/question_answering/question_answering.ipynb",
      "output_path": "/notebooks/tmp/question_answering/question_answering.ipynb",
      "parameters": {},
      "start_time": "2021-04-15T21:06:38.122812",
      "version": "2.3.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00e0cf870ffd45a7bf2f565e027d9584": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1286650862364c24a93fe103b3544104": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55ac394e7b0d49849f16237e5ce8af4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa97dca9d8494dc6a9117d748dbbc6f1",
            "placeholder": "​",
            "style": "IPY_MODEL_5e903ec09491478a989ac8e1c853d228",
            "value": " 296/296 [03:03&lt;00:00,  1.71it/s]"
          }
        },
        "5e903ec09491478a989ac8e1c853d228": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cf9b94c9344472ca3ed199d4aa093e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e7c0e3f9e545e48974b8d70c3254ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d394004ce654ff8b0f889cb8460ec88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5dcbe416a864c54929c0a60729ecee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1286650862364c24a93fe103b3544104",
            "max": 296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d394004ce654ff8b0f889cb8460ec88",
            "value": 296
          }
        },
        "c98040399b9f47a78a0580d523949485": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00e0cf870ffd45a7bf2f565e027d9584",
            "placeholder": "​",
            "style": "IPY_MODEL_85e7c0e3f9e545e48974b8d70c3254ea",
            "value": "100%"
          }
        },
        "e6e696f9356144ed9327f5d42b830c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c98040399b9f47a78a0580d523949485",
              "IPY_MODEL_a5dcbe416a864c54929c0a60729ecee2",
              "IPY_MODEL_55ac394e7b0d49849f16237e5ce8af4d"
            ],
            "layout": "IPY_MODEL_6cf9b94c9344472ca3ed199d4aa093e3"
          }
        },
        "fa97dca9d8494dc6a9117d748dbbc6f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
